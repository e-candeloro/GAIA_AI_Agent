{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f22091a",
   "metadata": {},
   "source": [
    "# LLM Agent Test Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb0f77",
   "metadata": {},
   "source": [
    "## 1. Test simple LLM with HF Inference via Langraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5030718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Annotated, List, Optional, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import ArxivLoader, WikipediaLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import (\n",
    "    ChatHuggingFace,\n",
    "    HuggingFaceEmbeddings,\n",
    "    HuggingFaceEndpoint,\n",
    ")\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# from supabase.client import Client, create_client\n",
    "from duckduckgo_search import DDGS\n",
    "import trafilatura  # pip install trafilatura\n",
    "from langchain_core.tools import tool\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796395fe",
   "metadata": {},
   "source": [
    "### Defining the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ae4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HF_LLM_API_TOKEN = os.getenv(\"HF_LLM_API_TOKEN\")\n",
    "\n",
    "# llm = ChatHuggingFace(llm=HuggingFaceEndpoint(\n",
    "#     repo_id=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "#     huggingfacehub_api_token=HF_LLM_API_TOKEN\n",
    "# ), verbose=True)\n",
    "\n",
    "# LLM_MODEL = \"llama3.2:1b\"  # Specify the LLM model to use\n",
    "\n",
    "# llm = ChatOllama(model=LLM_MODEL, format=\"json\", temperature=0)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2955c",
   "metadata": {},
   "source": [
    "### Defining the Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8094ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the agent.\"\"\"\n",
    "\n",
    "    # TODO: Add any additional state variables we need\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e58df",
   "metadata": {},
   "source": [
    "### Defining basics Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d3b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic calculator tools\n",
    "from langchain_core.tools import tool\n",
    "from typing import Sequence\n",
    "import math\n",
    "\n",
    "SAFE_GLOBALS = {\"__builtins__\": {}, \"math\": math}\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(expr: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate a basic arithmetic or math expression.\n",
    "\n",
    "    Accepted syntax\n",
    "    ---------------\n",
    "    • Literals: integers or floats (e.g. ``2``, ``3.14``)\n",
    "    • Operators: ``+``, ``-``, ``*``, ``/``, ``**``\n",
    "    • Unary minus (``-5``)\n",
    "    • Functions/consts from ``math`` (e.g. ``sin(0.5)``, ``pi``)\n",
    "    • Parentheses for grouping\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expr : str\n",
    "        The expression to evaluate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Result of the computation.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the expression contains unsupported syntax or names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \".\" in expr or \"__\" in expr:\n",
    "            raise ValueError(\"Attribute access not allowed\")\n",
    "        return eval(expr, SAFE_GLOBALS)\n",
    "    except (ValueError, SyntaxError, TypeError) as exc:\n",
    "        raise ValueError(f\"Invalid expression '{expr}': {exc}\") from exc\n",
    "\n",
    "\n",
    "# ────────────────────────  generic search utils  ───────────────────────\n",
    "\n",
    "_SEPARATOR = \"\\n\\n---\\n\\n\"\n",
    "\n",
    "\n",
    "def _format_docs(docs: Sequence, max_chars: int = 5000) -> str:\n",
    "    \"\"\"Uniformly format loader docs for the LLM / calling agent.\"\"\"\n",
    "    if not docs:\n",
    "        return \"No results found.\"\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        meta = doc.metadata\n",
    "        snippet = doc.page_content[:max_chars].strip()\n",
    "        chunks.append(\n",
    "            f'<Document source=\"{meta.get(\"source\")}\" page=\"{meta.get(\"page\", \"\")}\">\\n'\n",
    "            f\"{snippet}\\n</Document>\"\n",
    "        )\n",
    "    return _SEPARATOR.join(chunks)\n",
    "\n",
    "\n",
    "# ─────────────────────────  wiki_search  ──────────────────────────\n",
    "\n",
    "\n",
    "@tool\n",
    "def wiki_search(query: str) -> str:\n",
    "    \"\"\"Return up to 2 Wikipedia pages about *query*.\"\"\"\n",
    "    docs = WikipediaLoader(query=query, load_max_docs=2).load()\n",
    "    return _format_docs(docs)\n",
    "\n",
    "\n",
    "# ─────────────────────────  web_search   ──────────────────────────\n",
    "\n",
    "\n",
    "@tool\n",
    "def web_search(query: str, max_results: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Return up to `max_results` DuckDuckGo search results for *query*.\n",
    "\n",
    "    The output is formatted by `_format_docs`, so it matches the schema your\n",
    "    other tools already use.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    with DDGS() as ddgs:\n",
    "        for hit in ddgs.text(query, max_results=max_results):\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=hit.get(\"body\") or hit.get(\"snippet\") or \"\",\n",
    "                    metadata={\"source\": hit.get(\"href\") or hit.get(\"url\"), \"page\": \"\"},\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return _format_docs(docs)\n",
    "\n",
    "\n",
    "# ─────────────────────────  arxiv_search ──────────────────────────\n",
    "\n",
    "\n",
    "@tool\n",
    "def arxiv_search(query: str) -> str:\n",
    "    \"\"\"Return up to 3 recent ArXiv papers about *query*.\"\"\"\n",
    "    docs = ArxivLoader(query=query, load_max_docs=3).load()\n",
    "    return _format_docs(docs)\n",
    "\n",
    "\n",
    "# ---------- 1. Search → list of links -----------------------\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_webpage_links(url: str, same_domain_only: bool = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return all unique <a href=\"...\"> links found in the HTML at `url`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Page to scrape.\n",
    "    same_domain_only : bool, optional\n",
    "        If True, keep only links on the same domain as `url`.  Default = False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Absolute URLs, deduplicated and sorted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        html = requests.get(url, timeout=10).text\n",
    "    except Exception as exc:\n",
    "        return [f\"ERROR: fetch failed – {exc}\"]\n",
    "\n",
    "    base = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(url))\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    links: set[str] = set()\n",
    "    for tag in soup.find_all(\"a\", href=True):\n",
    "        href: str = tag[\"href\"].strip()\n",
    "        # Convert relative → absolute\n",
    "        full = urljoin(base, href)\n",
    "        if same_domain_only and urlparse(full).netloc != urlparse(url).netloc:\n",
    "            continue\n",
    "        links.add(full)\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "\n",
    "# ---------- 2. Browse → cleaned article text ----------------\n",
    "@tool\n",
    "def browse_webpage_link(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Download `url` and return the main readable text (no html, ads, nav bars).\n",
    "    Relies on trafilatura’s article extractor.\n",
    "    \"\"\"\n",
    "    raw = trafilatura.fetch_url(url)\n",
    "    if raw is None:\n",
    "        return \"🛑 Could not fetch the page.\"\n",
    "\n",
    "    text = trafilatura.extract(\n",
    "        raw,\n",
    "        include_comments=False,\n",
    "        include_tables=False,\n",
    "        include_links=False,\n",
    "    )\n",
    "    return text or \"🛑 Page fetched but no readable text found.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_links_for_match(\n",
    "    url: str,\n",
    "    keyword: str,\n",
    "    max_links: int = 100,\n",
    "    same_domain_only: bool = True,\n",
    "    case_sensitive: bool = False,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Search the content of up to `max_links` found on a webpage, and return URLs that contain the given keyword.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    url : str\n",
    "        The starting webpage to extract links from.\n",
    "    keyword : str\n",
    "        The keyword or phrase to match inside linked pages.\n",
    "    max_links : int, optional\n",
    "        Number of links to follow (default: 10).\n",
    "    same_domain_only : bool, optional\n",
    "        Only consider links from the same domain (default: True).\n",
    "    case_sensitive : bool, optional\n",
    "        Whether the keyword match should be case-sensitive.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list[str]\n",
    "        List of URLs whose content contains the keyword.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the tool's .func() to access base function\n",
    "    all_links = list_webpage_links.func(url=url, same_domain_only=same_domain_only)\n",
    "    matched_links = []\n",
    "\n",
    "    # Normalize keyword\n",
    "    kw = keyword if case_sensitive else keyword.lower()\n",
    "\n",
    "    for link in all_links[:max_links]:\n",
    "        try:\n",
    "            text = browse_webpage_link.func(link)\n",
    "            if not case_sensitive:\n",
    "                text = text.lower()\n",
    "            if kw in text:\n",
    "                matched_links.append(link)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return matched_links or [\"No matches found.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fe50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test web search\n",
    "# web_search = web_search.invoke(\n",
    "#     {\"query\": \"What is the capital of France?\"}\n",
    "# )\n",
    "# print(web_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a12f053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = list_webpage_links.invoke(\n",
    "#     {\"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "#      \"same_domain_only\": True}\n",
    "# )\n",
    "# print(len(links), \"links\")\n",
    "# print(links[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f249eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# browse_link = browse_webpage_link.invoke(\n",
    "#     {\"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\"}\n",
    "# )\n",
    "# print(\"Browse link result:\", browse_link[:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499121a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_links = search_links_for_match.invoke({\n",
    "#     \"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "#     \"keyword\": \"machine learning\",        # ← space not underscore\n",
    "#     \"max_links\": 30,                      # scan first 30 links\n",
    "#     \"same_domain_only\": True,\n",
    "#     \"case_sensitive\": False\n",
    "# })\n",
    "# print(\"Matched links:\", matched_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c4751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    calculator,\n",
    "    web_search,\n",
    "    wiki_search,\n",
    "    arxiv_search,\n",
    "    list_webpage_links,\n",
    "    browse_webpage_link,\n",
    "    search_links_for_match,\n",
    "]\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "550ae2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculator', 'web_search', 'wiki_search', 'arxiv_search', 'list_webpage_links', 'browse_webpage_link', 'search_links_for_match']\n"
     ]
    }
   ],
   "source": [
    "print([tool.name for tool in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a491fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are a helpful assistant tasked with answering questions using a set of tools.\n",
    "You have access to the following tools:\n",
    "{\", \".join([tool.name for tool in tools])}\n",
    "You can use these tools to search for information, perform calculations, and retrieve data from various sources.\n",
    "If the tool is not available, you can try to find the information online. You can also use your own knowledge to answer the question. \n",
    "You need to provide a step-by-step explanation of how you arrived at the answer.\n",
    "\n",
    "==========================\n",
    "Here is a few examples from humans, showing you how to answer the question step by step.\n",
    "\n",
    "Question 1: In terms of geographical distance between capital cities, which 2 countries are the furthest from each other within the ASEAN bloc according to wikipedia? Answer using a comma separated list, ordering the countries by alphabetical order.\n",
    "Steps:\n",
    "1. Search the web for \"ASEAN bloc\".\n",
    "2. Click the Wikipedia result for the ASEAN Free Trade Area.\n",
    "3. Scroll down to find the list of member states.\n",
    "4. Click into the Wikipedia pages for each member state, and note its capital.\n",
    "5. Search the web for the distance between the first two capitals. The results give travel distance, not geographic distance, which might affect the answer.\n",
    "6. Thinking it might be faster to judge the distance by looking at a map, search the web for \"ASEAN bloc\" and click into the images tab.\n",
    "7. View a map of the member countries. Since they're clustered together in an arrangement that's not very linear, it's difficult to judge distances by eye.\n",
    "8. Return to the Wikipedia page for each country. Click the GPS coordinates for each capital to get the coordinates in decimal notation.\n",
    "9. Place all these coordinates into a spreadsheet.\n",
    "10. Write formulas to calculate the distance between each capital.\n",
    "11. Write formula to get the largest distance value in the spreadsheet.\n",
    "12. Note which two capitals that value corresponds to: Jakarta and Naypyidaw.\n",
    "13. Return to the Wikipedia pages to see which countries those respective capitals belong to: Indonesia, Myanmar.\n",
    "Tools:\n",
    "1. Search engine\n",
    "2. Web browser\n",
    "3. Microsoft Excel / Google Sheets\n",
    "Final Answer: Indonesia, Myanmar\n",
    "\n",
    "Your Actions, to follow the human example, should be similar to the following:\n",
    "1. Use the wiki_search tool to search for the ASEAN Free Trade Area.\n",
    "2. Retrieve the list of member states from the Wikipedia page, and note their capitals if they are available. If not, use the web_search tool to find the capitals.\n",
    "3. Once you have the capitals lists, use the web_search tool to find the GPS coordinates of each capital city.\n",
    "3. Calculate the geographical distance between each pair of capitals. You can search for a formula to calculate the distance between two GPS coordinates, then use the calculator tool to perform the calculations.\n",
    "4. Identify the pair of capitals with the maximum distance.\n",
    "5. Provide the final answer in a comma-separated list, ordering the countries by alphabetical order.\n",
    "Final Answer: Indonesia, Myanmar\n",
    "==========================\n",
    "IMPORTANT: if you are not able to answer the question, even with the help of the tools, you MUST say \"I don't know\" instead of making up an answer!!!\n",
    "Report your thoughts, and finish your answer with the following template: [ANSWER]. The [ANSWER] should be a number OR as few words as possible OR a comma separated list of numbers and/or strings. If you are asked for a number, don't use comma to write your number neither use units such as $ or percent sign unless specified otherwise. If you are asked for a string, don't use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise. If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\n",
    "\n",
    "Now, please answer the following question step by step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78553b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(state: AgentState):\n",
    "    # System message\n",
    "    sys_msg = SystemMessage(content=SYSTEM_PROMPT)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6461b263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3XdcU1f/B/BzswcJkIRpQEAFZCgoSktdFSviqGLdWtfP3UWrtbXWqt3DPlqt1WK1VrSOinvUotYFooKCAiogStkQRhKy1++P+FAeDBE0N/eEe94v/8B7wz1f8OO5565zMZPJBBCEaBSiC0AQgIKIwAIFEYECCiICBRREBAooiAgUaEQXAB2t2iAp1yrlBqVcb9CbdFoHOL3FZFNoDIzDo3F4FA9fNtHlPAsMnUc0UzbpC7OainMV9VUaF3cGh0fl8Gh8AU2ncYDfD51FaajSKuV6GgMruasMCHMK6MXt1suJ6Lo6AAURmEym9ON1VY9Ubj6sgDCuuAeH6Iqei1ZtLM5tKr2vKi9SxYwRBvbhEV1Ru5A9iHevyc7tq4kZI+wz1JXoWmxM3qBLP16nlOuHv+7J5cM+BiN1EC8dqqXSwUtj3IguBEf11ZojmyuGTfPwDYa6pydvEP/+o0bgweg9yIXoQuzh6NbyF0YKPXxZRBfSJpIG8XhShU8QJ2IwKVJodnRLeXA/flAUpENGMp5HTD8u8e7GJlUKAQBjF3e5eb5BUqEhuhDLSBfEwltyAEDf2M52aNIeU5f7XjpUazLCuA8kXRAvptRGvkzGFJoFhDtdOSohugoLyBXEWxcagqP4bCcq0YUQJmKwS+GtJoVMT3QhrZEriI/yFC+OERBdBcEGjRdlX2wkuorWSBTER/kKGp1CpZLoR7bIN5ibmyYluorWSPSv8vCOwj+ca+dGP/zww6NHjz7DN77yyivl5eU4VAQYLIqbmFlepMJj48+MREGsr9F2s3sQ8/Pzn+G7KisrGxoacCjnscBIp7IiJX7bfwZkCaJWbZSUa9hOeF1yTUtLW7hw4YABA8aNG7d69WqJRAIAiIqKqqio+Oyzz4YMGQIAaGpq2rp166xZs8wfW79+vVqtNn97bGzs3r1758+fHxUVdfHixTFjxgAAxo4du3TpUjyq5TrTa8sgO6FoIof6ak3yF49w2vjdu3f79u27bdu2ysrKtLS0KVOmvPHGGyaTSa1W9+3b98iRI+aPbdu2LTo6OjU19caNG+fPn4+Pj//hhx/Mq+Li4iZOnPjdd99lZGTodLrLly/37du3rKwMp4KrS1T7vv8Hp40/G9hvyrAVhVTPdcbrh83OzmaxWHPnzqVQKJ6eniEhIUVFRU9+bMaMGbGxsf7+/ua/5uTkpKenv/322wAADMOcnZ2XLVuGU4WtcJ1pCilcZ3DIEkSjETDYeI1DIiIi1Gp1YmJidHT0oEGDfHx8oqKinvwYnU6/evXq6tWrCwoK9Ho9AEAg+PdcUkhICE7lPYlCwxgsuEZlcFWDHy6fKq3V4bTx4ODgjRs3urm5bdq0KSEhYcmSJTk5OU9+bNOmTUlJSQkJCUeOHMnMzJwzZ07LtQwGA6fynqRo1FNpmN2aaw+yBJHDpynxvJwQExOzatWq48ePr1mzRiqVJiYmmvu8ZiaTKSUlZfLkyQkJCZ6engAAuVyOXz3WKWR62G6VJUsQ2VyqqAtTrzPisfGsrKz09HQAgJub2+jRo5cuXSqXyysrK1t+RqfTqVQqd3d381+1Wu2lS5fwKKY9NEqjuw+TqNYtIksQAQBsJ2rxHQUeW87JyVm+fPmhQ4caGhpyc3P37dvn5ubm5eXFZDLd3d0zMjIyMzMpFIqfn9+xY8fKysoaGxs//fTTiIgImUymUFgoyc/PDwCQmpqam5uLR8EFN+UeXeG6SZZEQfQP4z7MxSWIM2bMSEhIWLdu3SuvvLJgwQIul5uUlESj0QAAc+fOvXHjxtKlS1Uq1ZdffslisSZMmDBu3Lj+/fu/+eabLBZr2LBhFRUVrTYoFovHjBmzdevWTZs24VHwo3ylf6i9z+1bR6I7tLUa48ntlQlLuhBdCMH+ua8svtM0ZII70YX8DxL1iAwmxV3MvHkex0tnDiH9mCT0RWeiq2gNrkMnvMWMFm5e9qCtJ0eNRuPQoUMtrtJqtXQ6HcMsnPIICAjYsWOHrSt9LDs7OzExsaMlBQYGJiUlWfyugptyVw+GWxe4jlTItWs2y7nUaDSaIodYzmJbp1Q0Gg2TafkfD8MwJycc51R4hpIoFAqXa3kIeHJ7xcAEN76AbtMabYB0QQQAnNpRGRTFc6wZOWwC5h+cRGPEZiPnel09UVdTqia6ELu6mFIr9GLAmUKS9oiPr3P8UPbCKKGjz3TTThdTat19mT378YkupE1k7BHNA7sJiT43/mrIy4DupnnbMplMR7eU8wU0mFNI3h6x2dWTkod5ypjRQr8QuE7w2kRman1ehuzlSe6+QbB3/GQPIgCgrkKTfqKOyaZ06cH2D+VyeA5/Squ2TFNyV5F1rqHXQJfoeAGFAteNNhahID5W/kB1/4b8YZ7C1YMu8GBwnWlcPo3rTDUYiK6sHTDMJK/XK2QGk9FUcLOJxaV07+3Ua6ALbDcdWoGC2FrVI1VtuVYh1StkegoFU8ptmUSVSlVcXBwaGmrDbQIAnFxpwAS4fCrPlebdjc1zhe404VOhINrVgwcPVqxYceDAAaILgY7DdN1I54aCiEABBRGBAgoiAgUURAQKKIgIFFAQESigICJQQEFEoICCiEABBRGBAgoiAgUURAQKKIgIFFAQESigICJQQEFEoICCiEABBRGBAgoiAgUURAQKKIgIFFAQESigINoVhmHNb7hAWkJBtCuTyVRTU0N0FTBCQUSggIKIQAEFEYECCiICBRREBAooiAgUUBARKKAgIlBAQUSggIKIQAEFEYECCiICBRREBAooiAgUUBARKKAX/tjDlClTlEolAECr1dbV1Xl5eZlfQX/mzBmiS4MF6hHtYezYsVVVVRUVFRKJxGQyVVRUVFRU8Hg8ouuCCAqiPUyZMsXX17flEgzDBgwYQFxF0EFBtAcMw8aPH0+lUpuXdO3adfLkyYQWBRcURDuZNGmSj4+P+WsMwwYPHmweKSJmKIh2QqPRpkyZwmQyAQBisXjChAlEVwQXFET7GT9+vFgsBgDExMSg7rAVGtEF2JuqyVBXodVqjYS0PiZ2XqoxdUj/ycW5CiLaNzm50AQeDBodug6IROcR9VrjX7uryx+oxIFcnZqYIBKLzqA01moNemNgX17/OAHR5fwPsgRRozKkbCzvFy/y7MohuhbiZf4lodLAoAQR0YX8C7ouGif715UOmeSFUmgWNVxkMmHpJ+qILuRfpAhibro0oDePJ6ATXQhE+sQKK4pVTTI90YU8RoogVpWoOXyUwtYwDGuo0hJdxWOkCKJWbeQLURBbE3gxFY0Goqt4jBRBVCuMJjIeJT+FVm00GGE5VCVFEBH4oSAiUEBBRKCAgohAAQURgQIKIgIFFEQECiiICBRQEBEooCAiUEBBRKCAgoiv4uKil2Ojbt++RXQhsENBxJeLi+vM1+e5u3ta+czDhw+mTBv9nA0lvPZKRWX5c26EQKR7eMrOBALhnNmLrH/mfkH+c7ZSVVXZ2NjwnBshFgqiZVevXj7/95nbd27JZNKewWGvvz4vMiLKvCrjWtr+/bvu3c8TCERhYb0XzHtLKBS1tby4uOj/5k/5Yf22Xr0i5U3yX3duvZZxpaGxPigwZNiw+FEjx/26c+uu5F8AAC/HRi1Z/O7ECdPbavrwkQPJu3/Z8J+k1WuXP3pUHBDQfeKE6SPixtzKznxv6SIAwPQZY6dNnT1/3ptE//KeBdo1W6BWq7/46mONRvPhB2u//GKDr6/fyo/fra+vAwAUFN5b8dE7kZH9du44+PZbyx88KPjm2zVWlrf07bdr8/NuJyau2LnjYM+eYes3fJWXd3vO7EVTJs/08PD8+1zmxAnTrTRNp9ObmuQbN337/tJV58/eGDxo2LfffVpdXRUZEfXVFxsAAHt2H3XQFKIe0TIWi/VL0j42m+3s7AIA6BkcdvTYwTu52YMHxebeyWaxWDOmz6VQKB4ensFBIcUPiwAAbS1vKef2zSmTZ/aLegEAsGD+W4MHD3Pmu7S/aQCATqebNXNBSEg4ACBu+Ohfd24tKrrv4WFtAOooUBAtUyoVv2z/MTsnq65OYl5iHoSFhUeo1eoVKxOj+ka/+OIgcRcf836zreUthYdHHPhjt1Ta2LtXn379XgwK7Nmhps2Cg0PNX/B4fABAU5Mcn1+AvaFdswXV1VXvvDtPp9OtWvnlX39eTT2T0bwqsEfw119tFAndkrZten1mwrL3l+Tm5lhZ3tIHy9dMeG3ajcyrK1e9N/61V3b8ukWvb/0QnZWmzTAMw+3nJhLqES24cDFVq9V++MFaNpvdqkMCAET3j4nuHzNn9qKsrGsph/Z+tDLxUEoqjUazuLzlN/J5/BnT506fNic3N+fylb+Td293cuJNmjij/U13YiiIFshkUh6Pb44CAODipXPNq7KzszRaTXT/GJHILS5utKend+J7C6qqKyW1NRaXN3+jVCY9d+7PkfFjWSxWeHhEeHhEUdH9gsJ77W+6c0O7ZgsCAnrU1UmOHU/R6/XXrqffvHnd2dmlpqYKAJCbl7Nm7fLjJw41Njbk3809dHifSOTm6eHV1vLmbdKotN92Ja359IPc3Jz6+rq//jpZWHQvPCwCACAW+9bVSa5cuVBaWmKlaSt8fP0AABcupJaUPMT/14ML6po1rc8ydD53r8s9urKdXNr7aHOAf3ej0XAw5fefkzZKpQ1L31upUin3H0iur5fMmb1ILpft3rP99707z549FRjY8/33P3FxcQ0ODrW4vKGh/tjxg/EjXvXx8Q3pGX7hYuqe33898Mfu8orSma/PHzVyHIZhQoHo/v383/ft5PNdxidMbqtpodDt6tXLM1+fR6FQzEfQv+/9dcBLQ7p3D+Tz+NXVlYcO7wMYFt0/pp0/ZmmBgi+guYuZz/GrtRlSTMJ06Mfy8IECTz820YXAJf14jbg7K/QFPtGFALRrRmCBgohAAQURgQIKIgIFFEQECiiICBRQEBEooCAiUEBBRKCAgohAAQURgQIKIgIFFEQECqQIorOIBkhwk1FHMVkUBhOWBw9IEUQ2l1pbriG6CuiUFykFHgyiq3iMFEHsGsptrIXlFUuQUCsNbCeq0BuKu2LJEsQuAWyBOy3jRA3RhUDk7O6KAeMgejspKe7QNss821BTqvHuxhF1YVFppPgf2AqGmeSNerlEe+20ZMoyH1do9svkCiIA4NFdRUFWk0phaGzxMkSNVkuhUOg0ezzQaDSZdDodk4FXAhRKJYZhVCqV8l8tD0YYHCqDiXkFsPoPF9AYcP1XJFcQWzEYDEVFRRcuXFi4cKF9Wnzw4MGKFSsOHDiA0/ZXrFhx5swZDMNcXV2dnJyYTKa3t3dgYODixYtxatFWyBvEXbt2jRo1isvlslgsuzUql8uzsrKGDBmC0/bv3buXmJgokUhaLjQajV5eXidPnsSpUZuAq3+2m5SUlIaGBqFQaM8UAgB4PB5+KQQABAcH9+zZekodLpcLeQrJGMTz588DAF566aV33nnH/q3X1tb+9NNPuDYxbdo0V1fX5r9SKJTLly/j2qJNkCuIX3/9dXFxMQDA05OYqdxkMtmFCxdwbaJfv37dunUzj7iMRmNAQMDRo0dxbdEmSDHTAwCgqKhIIBBwudxRo0YRWAadTheLxX5+fri2wuFwrl+/rtFoxGJxSkrKgQMH0tLSBg4ciGujz4kUBysrVqyIjY0dNmwY0YXYz/Tp06urq8+ePWv+a0pKyuHDh3fv3k10XW0zdWpyuby0tPTMmTNEF/JYTU3N5s2bCWk6Pz+/b9++ubm5hLT+VJ15jPjZZ59JJBKxWDx8+HCia3nMDmPEtvTs2TMzM/Obb745ePAgIQVY12mDmJKSEh4ejvdorKPc3d2XLFlCYAG7du0qLCxcu3YtgTVY1AnHiElJSQsWLNBqtQzcrqQ5umPHju3Zsyc5ORmeX1Fn6xE/+eQTFxcXAAA8v+KW7HAesT1effXVL774YvDgwdnZ2UTX8l9ED1Jt5sKFCyaTqba2luhCrCkqKpo4cSLRVfxr7ty5e/bsIboKU+c5WJk+fbp5un2RCKJ77J5E+Bixle3bt1dWVn788cdEF+L4Y8SysjJ3d/fi4uLg4GCia3FUp0+f3rZtW3JyMpfLJaoGB+4R9Xr9/Pnz1Wo1g8FwlBRCMkZsJT4+fv369fHx8Tdu3CCqBkcNoslkSktLW7x4cffu3YmupQMIPI9oXdeuXS9durR9+/bffvuNkAIcL4hGo/Hdd981mUyDBw/u06cP0eV0DGxjxFa2bt0qlUqXL19u/6Ydb4y4evXq2NjYQYMGEV1Ip3Xu3LkNGzYkJyebT4TZCdGH7R2wc+dOokt4XgRea+6Q8vLyoUOHXrlyxW4tOsyuecSIEWFhYURX8bygHSO24u3tfe7cuf379//yyy/2adEBds03b97s06ePWq228239eMD7mRWb27JlS0FBwfr16/FuCOoeUaFQxMXF8fl88xu1iS7HBvB+ZsXmFi9enJCQEBcXV1OD8/QEdhsEdJRcLi8oKID8kl1HOcoYsZXa2toRI0ZkZ2fj1wSkPeKhQ4du3rzZo0cPyC/ZdRSLxbp16xbRVXSYSCQ6ffr05s2by8vLcWoC0vc1FxYW6nQ6oquwPR6P99NPP6lUKgzDHG6wcfPmTW9vb5w2DmmPuGjRotGjRxNdBS7odDqbzd6/f39lZWU7Pg6Le/fuBQUFme8swQOkQXR2dibwArwdzJo1KzExkegqOuDu3btPPrpvQ5AG8eeffz5x4gTRVeBr//79AIDS0lKiC2mX/Pz8kJAQ/LYPaRClUqlCoSC6Cnu4ePFiVlYW0VU8Hd49IqQntKVSKY1G69x752aff/45DLemWhcVFZWZmYnf9iHtETv9GLElcwozMjKILqRN+fn5uHaH8AaRDGPEVsrKys6cOUN0FZbhvV+GN4jkGSM2mzBhgkwmI7oKy/A+UoE3iAsXLuys5xGtmDhxIgBg7969RBfSGnl7RFKNEVsRCoVQzQpiNBoLCwuDgoJwbQXSIJJwjNhs+PDhUM2UYof9MrxBJOEYsaWoqCjzrBVEFwLss1+GN4jkHCO2kpCQsGfPHqKrsFMQIb37xtnZmegSiBcZGenh4UF0FSA/P3/q1Kl4twJpj0jmMWJL5tuuEhISiCpAr9c/fPiwR48eeDcEaRBJPkZsZevWrcnJyS2X2G3qUfscqaBrzQ5Dq9VqtVoqlcpms0eOHFldXR0XF/fll1/i3e7+/ftLSkrs8Mg9GiM6BgaDwWAwBgwY4OLiUlNTg2FYXl5efX29QCDAtd38/Px+/frh2oQZpLtmNEa0SCgUVlVVmb+ur6+3w5t87HPIDG8Q0RjxSa+99lrLZ5cUCkVqaiquLWq12tLS0m7duuHaihmku+aFCxfS7PLeWkeRkJBQUlJifqWZeQmFQikpKSkuLg4ICMCpUbsdqcDbI5L5WrNFhw8fTkhI8PPzM0+MZDQaAQDV1dW47p3ttl+Gt0f8+eefu3Tpgi6utLRq1SoAwO3bty9fvnz58uW6ujppg/LiuevjX52OU4v38/6JjIyUN+ifeQsmE+AL2pUxuE7fDB06VCqVNpeEYZjJZPL09Dx16hTRpcElM7X+9pUGI6bXa0xs3J6P1uv1VBrteR4gdfVilhcqu/fmRo8U8gV0K5+Eq0eMiYk5depU8zDIPBIaM2YMoUVB58/fqpwE9Pi5vk4u1v5pIaHXGRtrtH/8UDb+jS6u7m2+cwSuMeLUqVNbzSUgFovtcKHTgZzeWeXqyew9SOgQKQQA0OgUURfWpPf8D28ul9W3OXsHXEEMDQ1tOQkihmEjRoyw67ylcHuUr2CwqSEvuLbjs9B5ebJXxqn6ttbCFUQAwMyZM5snXhKLxZMmTSK6IojUlGroTOj+ydrJ1YNZlC1vay10P1VISEivXr3MX8fHx7u6OuT/fpxolAaRF5PoKp4RlYb5BnEba7UW10IXRADA7NmzhUKhp6cn6g5bUcgMekeeI62+WtvWNE7Pe9Rc8UAplegVcr1SZjAagF5vfM4NAgAAEA4IWszlcjNPawCofv7NMdkUDGAcPpXDpwq9mW7ejtqpdGLPGMSSu4qCm03FuQpXT7bJhFHpVAqdSqFSbXVWMqzXEACA3EZXm5uUmNFgMJTrDVq1Ti3VqQ3denGDo3geXR1shsJOrMNBrHyounS4js5hYDRmtxddaXQqPoXhSKvS10kUF480sDlg4DihixuML9Qlm44F8eze2opitdBfwHV14L6EwaYJfJwBALIaRcqmip79eTGjhUQXRXbtPVjR64w7Py1RG5i+fbwdOoUt8d253V70qamiHN6M19TQSDu1K4gGvSlpRbFXiIeTsBPeEePShU935u9b5xgTZnZWTw+i0WjasvxBSKw/k+sY15SegZOQw+8i+O3zEqILIa+nB3HPV//0iOlil2KIxHFhCXxcTm53pAnWO5OnBPFCisTFx4XJJcVxJc/dSQeY2RcbiS6EjKwFsa5C8zBXwXNzsmM9BHPxdr5yRALVPZokYS2Il47UifzxfVoRQp6BrpeP1BFdBem0GcSqRyq9gcJz49i3nvbKvnN22aroJkWDzbcs8nMpL9ZoVAabb9lBjRs/bFcy7i/LbTOIRTkKjNppD5OfAqM8ylMSXYRtrP30w1OnjxJdxdO1GcQHtxU8d0i7Q7xxBNzC7Caiq7CN+/fziS6hXSxf4muo0bJ5dPwOlh/9c/uvv38pLct34rr2DBow/OV5LBYXAJCW8UfqxR2L527ZtW9FdU2xl0f3QTFT+/V5/CzfiT83ZeacYjI4kb3i3EW+ONUGAOC7cyrzIJ1XvUNejo0CAHy37rMtW9cfP3oBAJCWdvG3XUkl/zx0dnbp3j3onbc+8PDwNH/YyqpmGdfS9u/fde9+nkAgCgvrvWDeW0KhbV4fa7lHbGrUq1U2uaHLAkld6c8739LpNG8u+GXWtG8qqwu37FhsMOgBAFQaXaWSHzm5btK4j777NKNX2NADRz5vaKwCAKRfT0m/fnD8qPffWfir0NU79e/tOJVnfkShqUGnkD37Y5SQ+PNUGgDg/WWrzCnMzLr2yZr3hw8fdWDfqdWrvq6urtyw8WvzJ62salZQeG/FR+9ERvbbuePg228tf/Cg4Jtv19iqVMtBVMoMVNxuq7mZ8yeNSp899RsPNz9P94CJY1eWV97PvXvRvNZg0L3y8ryuPuEYhkVFjDKZTOWVBQCAK1cP9AqN7RU2lMPh9+szuntAFE7lmTFYVIXU4YPYyo5ftwwaOHTCa9OcnV1CQ3stWfxeRsaVe/fzra9qlnsnm8VizZg+18PDM7p/zPffbZk6dbatamsjiHI9lYHXk6aP/rntIw7hch8/EiVw9RIKxA9Lsps/4Nsl1PwFh80HAKjUcpPJJKkv9XD3b/6M2DsYp/LM6Gyq0vF7xFaKiwuDg0Ob/xoUGAIAuHcvz/qqZmHhEWq1esXKxD8O7ikrL3V2domMsFl30GbaMIDXSV2Vuqm0PH/ZquiWC2Xyf0/dPXk3uVqjMBoNTOa/B08MBhun8syMBgBwezcxIZqamjQaDZP5751THA4HAKBUKqysarmFwB7BX3+18dKlc0nbNv20ZX3fPv1nz1oYFtbbJuVZDiKHTzPo1DZp4Ek8ntC/a0Tc0AUtF3K51iZEZDG5FApV16IkjRbf0ysGrYHLh2v2gefEYrEAAGq1qnmJQqkAAAgFIiurWm0kun9MdP+YObMXZWVdSzm096OViYcPnaVSbTCKs7xr5vCoBh1eZ3S9PXo0SqsC/CK7B/Q1/3FycnUXWXuzCIZhri5ej/6507zk7v00nMoz06oNHL7j3XxuBY1GCwrsmZd3u3mJ+euAbj2srGq5hezsrGvX0wEAIpFbXNzoN5YslTfJJZJam5RnOYh8AY3OwGvHNChmqtFoPHZ6vVarrqktOXHmx+9/nFZZXWT9u3qHDbuT/3f2nbMAgPOXd5WU5eJUnvnONycXWifoEZlMppube2Zmxq3sTL1enzBu8pW0Cykpe2Vy2a3szJ+2/KdPZL8e3YMAAFZWNcvNy1mzdvnxE4caGxvy7+YeOrxPJHITidxsUqrl37WziKFXG9RyLYtn+1OJHA5/2Zu//305ecPWWTW1j3zFoRPHrXzqwcewwXMUioYjp77ffWClf9eIV+MTf//jE5zuTpBVK1zdO8lVpenT5v66c+v1G+l7fz8xfPioWknN/j+Sf/zpew8Pz6i+L8yf96b5Y1ZWNZs0cUZjY8OPm9f9Z/2XDAZj6Mtx6/+TZJP9srXZwK6erCt7ZHILIOPz7RV5Nf1inXpE8ogupLU/f6vy7ubkH+6o90Md3lQydpG3s8jCf/I2L/F178016Tvb+Yt2wjCDf2gnfCgCZm0Og9zELDbHJK1WOHtY/idplNas+9HyPF1sppNKY/laradbwJsLtj1rtRZ8/EVsW6sMBj2VauEH9BWHLpi1sa3vqi1u8A9h0xgwzoHRiVkbjw8aLzq4obytIPKcBO8tSba4SqtVMxiWn/SjUGx8BNBWDQAArU7DoFuY1IFGa3PgazQYax9KJ75hj+nLkZasxcJZSO8Z7VRXK+e5WRgtUak0gau3pe+zK9vWIKuUDplom6v4SIc8ZQcUM1qklDQpG/E6uQ0VaaXMiWsMiUbvGiLA00dCk98T/3OrSqfu5AcujVVNqvqmYdPciS6EpNo1JF/4TUBhWmkn7helVU1ArZiyzIfoQsirXUHEMGzJuu6y8npZdZszfjquhtIGBqYat5j48S6ZdeAkxZRlPkKhoTijTFbTSV5O1lAuu3ehxD+IFj+79a3IiJ117GTKS2OEIdG8S4frJA+UJiqd78Z1xHlIVDKNvFZp1GhE3vSRa7oy2Z3q5gYH1eGzeq7ujLELvaoeqQuzmx7crmZyaEYjRmVQqXQGjTZeAAABMUlEQVQqhUYFuN3F+DwwDNPrDEatXq81aFU6JpvSI8IpsI8bmhkRHs94etnTj+Xpxxo4TlRfpZVKdAqZXiHVG/RGgx7GIDJYGIVK4fI5HD5V1IXh5Ox4vXin97zXOQSeDIEn6leQ54WuqDoSrjPNoSc9EHgy2xq8oSA6EjaXIinXEF3FM9JpjWUFCmeR5f0nCqIj8ejK0mkcdVKe+iqNlVs8URAdiU8gB8PArfMOOVnZ+d8rXnq1zUnz4XpfM9Ielw7V6nSmbr34Qm8HmFVfIdNLazV/76t6faUvt+3zFSiIDin3qjQvXaZWGjS4zQxjE25dmI01Wv9w7ktjRNZfZ4mC6MBMJqBVQx1Ek9HE4rbrwhUKIgIFdLCCQAEFEYECCiICBRREBAooiAgUUBARKPw/UQ7qSwMCYJAAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show the butler's thought process\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6179a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [HumanMessage(content=\"Divide 6790 by 5\")]\n",
    "# messages = react_graph.invoke({\"messages\": messages})\n",
    "\n",
    "# # Show the messages\n",
    "# for m in messages['messages']:\n",
    "#     m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d66c9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What writer is quoted by Merriam-Webster for the Word of the Day from June 27, 2022?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  web_search (97ba28af-fcb3-444a-b964-866d03df628a)\n",
      " Call ID: 97ba28af-fcb3-444a-b964-866d03df628a\n",
      "  Args:\n",
      "    query: Merriam-Webster Word of the Day June 27 2022\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: web_search\n",
      "\n",
      "<Document source=\"https://www.merriam-webster.com/word-of-the-day/candidate-2022-06-27\" page=\"\">\n",
      "June 27, 2022 | extreme patriotism or nationalism Jingoism originated during the Russo-Turkish War of 1877-1878, when many British citizens were hostile toward Russia and felt Britain should intervene\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"https://www.merriam-webster.com/word-of-the-day/calendar\" page=\"\">\n",
      "Learn a new word every day! Follow Merriam-Webster for the most trusted Word of the Day, trending info, word games, and more. ... June 10, 2025 . minutia play . a small or minor detail. June 09, 2025 . eloquent ... May 27 interminable; May 28 fiasco; May 29 nascent; May 30 gust; May 31 opportune; April 2025. Apr 01 cynosure;\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"https://en.wiktionary.org/wiki/Wiktionary:Word_of_the_day/2022/June_27\" page=\"\">\n",
      "Pages for logged out editors learn more. Contributions; Talk; Wiktionary: Word of the day/2022/June 27\n",
      "</Document>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The Word of the Day for June 27, 2022, according to Merriam-Webster, was \"jingoism.\" The provided search results do not explicitly state which writer is quoted. I will browse the link to find the specific quote and its author.\n",
      "Tool Calls:\n",
      "  browse_webpage_link (c6de65fe-ef36-43bd-991f-76277be7e671)\n",
      " Call ID: c6de65fe-ef36-43bd-991f-76277be7e671\n",
      "  Args:\n",
      "    url: https://www.merriam-webster.com/word-of-the-day/candidate-2022-06-27\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: browse_webpage_link\n",
      "\n",
      "Word of the Day\n",
      ": June 27, 2022jingoism\n",
      "playWhat It Means\n",
      "Jingoism is excessive patriotism or nationalism, especially when marked by a belligerent foreign policy.\n",
      "// When the war began many people were caught up in a wave of jingoism.\n",
      "jingoism in Context\n",
      "\"War is bad for culture. Not least of all because it turns our cultural institutions into bastions of jingoism.\" — Annie Levin, The New York Observer, 7 Mar. 2022\n",
      "Did You Know?\n",
      "Jingoism originated during the Russo-Turkish War of 1877-1878, when many British citizens were hostile toward Russia and felt Britain should intervene in the conflict. Supporters of the cause expressed their sentiments in a music-hall ditty with this refrain:\n",
      "We don't want to fight, yet by jingo if we do,\n",
      "We've got the ships, we've got the men,\n",
      "We've got the money, too!\n",
      "Someone holding the attitude implied in the song became known as a jingo or jingoist, and the attitude itself was dubbed jingoism. The jingo in the tune is probably a euphemism for Jesus.\n",
      "Quiz\n",
      "Unscramble the letters to find a word that means \"a supporter of a war or warlike policy\": KWAH\n",
      "VIEW THE ANSWERPodcast\n",
      "More Words of the Day\n",
      "-\n",
      "Jun 13\n",
      "rambunctious\n",
      "-\n",
      "Jun 12\n",
      "impute\n",
      "-\n",
      "Jun 11\n",
      "debilitating\n",
      "-\n",
      "Jun 10\n",
      "minutia\n",
      "-\n",
      "Jun 09\n",
      "eloquent\n",
      "-\n",
      "Jun 08\n",
      "cataract\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The writer quoted by Merriam-Webster for the Word of the Day from June 27, 2022, is Annie Levin.\n",
      "[ANSWER] Annie Levin\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"What writer is quoted by Merriam-Webster for the Word of the Day from June 27, 2022?\"\n",
    "    )\n",
    "]\n",
    "messages = react_graph.invoke({\"messages\": messages}, config={\"debug\": True})\n",
    "\n",
    "# Show the messages\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b6256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c88bfa7a",
   "metadata": {},
   "source": [
    "## 2. Structured Output LLM Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6acbffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain.output_parsers import OutputFixingParser  # optional but handy\n",
    "\n",
    "# ─────────────────────── 1. LLM backend ─────────────────────────── #\n",
    "load_dotenv()\n",
    "llm = ChatHuggingFace(\n",
    "    llm=HuggingFaceEndpoint(\n",
    "        repo_id=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "        huggingfacehub_api_token=os.getenv(\"HF_LLM_API_TOKEN\"),\n",
    "    ),\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# ─────────────────────── 2. Pydantic schema ─────────────────────── #\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Structured reply returned to the caller.\"\"\"\n",
    "\n",
    "    final_answer: str = Field(..., description=\"The concise answer for the user\")\n",
    "    reasoning: List[str] = Field(..., description=\"Chronological chain-of-thought\")\n",
    "\n",
    "\n",
    "# ───────────── 3. Parser (+ auto-repair wrapper, optional) ────────── #\n",
    "# :contentReference[oaicite:0]{index=0}\n",
    "base_parser = JsonOutputParser(pydantic_object=Answer)\n",
    "parser = OutputFixingParser.from_llm(llm=llm, parser=base_parser, max_retries=2)\n",
    "# :contentReference[oaicite:1]{index=1}\n",
    "fmt_instructions = parser.get_format_instructions()\n",
    "\n",
    "# ─────────────────────── 4. Prompt template ──────────────────────── #\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are a helpful assistant that answers questions using a set of tools.\n",
    "\n",
    "• Tools available: {\", \".join(t.name for t in tools)}\n",
    "• Use a tool whenever the information or computation is not already in your head.\n",
    "• After thinking, respond with EITHER:\n",
    "  • a single JSON *tool-call* (if you still need external info), OR\n",
    "  • the **final JSON answer** described below.\n",
    "  • Do NOT wrap the JSON in triple back-ticks.\n",
    "• If you cannot answer even with tools, set \"final_answer\" to \"I don't know\".\n",
    "\n",
    "Worked example\n",
    "--------------\n",
    "Q: Which ASEAN capitals are farthest apart?\n",
    "→ Tool calls (look up capitals, compute distances)  \n",
    "→ Final JSON:\n",
    "{{\n",
    "  \"final_answer\": \"Indonesia, Myanmar\",\n",
    "  \"reasoning\": [\n",
    "    \"Gather capitals from Wikipedia\",\n",
    "    \"Compute pairwise distances\",\n",
    "    \"Select maximum distance pair\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_TMPL = \"\"\"{system_prompt}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Conversation so far:\n",
    "{history}\n",
    "\n",
    "User: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TMPL,\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    partial_variables={\n",
    "        \"system_prompt\": SYSTEM_PROMPT,\n",
    "        \"format_instructions\": fmt_instructions,\n",
    "    },\n",
    ")\n",
    "\n",
    "# ─────────────────────── 5. Agent state ──────────────────────────── #\n",
    "\n",
    "\n",
    "# ---------------- State -----------------\n",
    "class AgentState(BaseModel):\n",
    "    messages: List[AnyMessage] = Field(\n",
    "        default_factory=list, json_schema_extra={\"x_add\": add_messages}\n",
    "    )\n",
    "    reasoning: List[str] = Field(default_factory=list)  # NEW\n",
    "    final_answer: str | None = None  # NEW\n",
    "    # NEW\n",
    "\n",
    "\n",
    "# ─────────────────────── 6. Assistant node ───────────────────────── #\n",
    "\n",
    "\n",
    "def assistant_node(state: AgentState):\n",
    "    history = \"\\n\".join(\n",
    "        m.content for m in state.messages if isinstance(m, HumanMessage)\n",
    "    )\n",
    "    question = state.messages[-1].content\n",
    "    full_prompt = prompt.format(\n",
    "        system=SYSTEM_PROMPT, history=history, question=question\n",
    "    )\n",
    "\n",
    "    raw = llm.invoke(full_prompt)\n",
    "    parsed = parser.parse(raw.content)\n",
    "\n",
    "    # If you aren’t sure which type you get, normalise:\n",
    "    if isinstance(parsed, dict):\n",
    "        structured = Answer.model_validate(parsed)\n",
    "    else:  # it’s already an Answer\n",
    "        structured = parsed\n",
    "\n",
    "    return {\n",
    "        \"messages\": [raw],\n",
    "        \"final_answer\": structured.final_answer,\n",
    "        \"reasoning\": structured.reasoning,\n",
    "    }\n",
    "\n",
    "\n",
    "# ─────────────────────── 7. Summariser node ─────────────────────── #\n",
    "\n",
    "\n",
    "def summariser(state: AgentState) -> Answer:\n",
    "    return Answer(final_answer=state.final_answer, reasoning=state.reasoning)\n",
    "\n",
    "\n",
    "# ─────────────────────── 8. Build LangGraph ─────────────────────── #\n",
    "g = StateGraph(AgentState)\n",
    "g.add_node(\"assistant\", assistant_node)\n",
    "g.add_node(\"tools\", ToolNode(tools))  # <-- your tools list goes here\n",
    "g.add_node(\"summarise\", summariser)\n",
    "\n",
    "g.add_edge(START, \"assistant\")\n",
    "g.add_conditional_edges(\"assistant\", tools_condition, (\"tools\", \"summarise\"))\n",
    "g.add_edge(\"tools\", \"assistant\")\n",
    "g.set_finish_point(\"summarise\")\n",
    "\n",
    "agent = g.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cfe85e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import cmath\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import requests\n",
    "import trafilatura\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from duckduckgo_search import DDGS\n",
    "from groq import Groq\n",
    "from langchain.agents import tool\n",
    "from langchain_community.document_loaders import ArxivLoader, WikipediaLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import (\n",
    "    ChatHuggingFace,\n",
    "    HuggingFaceEmbeddings,\n",
    "    HuggingFaceEndpoint,\n",
    ")\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from markitdown import MarkItDown\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter, ImageFont\n",
    "\n",
    "load_dotenv()\n",
    "### =============== MATHEMATICAL TOOLS =============== ###\n",
    "\n",
    "SAFE_GLOBALS = {\"__builtins__\": {}, \"math\": math}\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "# one Groq client reused for all calls\n",
    "_GROQ_CLIENT = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "TEMP_DIR = os.getenv(\"TEMP_DIR\", \"./tmp\")  # Default temp directory\n",
    "QUESTIONS_FILES_DIR = os.path.join(TEMP_DIR, \"questions_files\")\n",
    "os.makedirs(QUESTIONS_FILES_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(expr: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate a basic arithmetic or math expression.\n",
    "\n",
    "    Accepted syntax\n",
    "    ---------------\n",
    "    • Literals: integers or floats (e.g. ``2``, ``3.14``)\n",
    "    • Operators: ``+``, ``-``, ``*``, ``/``, ``**``\n",
    "    • Unary minus (``-5``)\n",
    "    • Functions/consts from ``math`` (e.g. ``sin(0.5)``, ``pi``)\n",
    "    • Parentheses for grouping\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expr : str\n",
    "        The expression to evaluate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Result of the computation.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the expression contains unsupported syntax or names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \".\" in expr or \"__\" in expr:\n",
    "            raise ValueError(\"Attribute access not allowed\")\n",
    "        return eval(expr, SAFE_GLOBALS)\n",
    "    except (ValueError, SyntaxError, TypeError) as exc:\n",
    "        raise ValueError(f\"Invalid expression '{expr}': {exc}\") from exc\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Multiplies two numbers.\n",
    "    Args:\n",
    "        a (float): the first number\n",
    "        b (float): the second number\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Adds two numbers.\n",
    "    Args:\n",
    "        a (float): the first number\n",
    "        b (float): the second number\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> int:\n",
    "    \"\"\"\n",
    "    Subtracts two numbers.\n",
    "    Args:\n",
    "        a (float): the first number\n",
    "        b (float): the second number\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Divides two numbers.\n",
    "    Args:\n",
    "        a (float): the first float number\n",
    "        b (float): the second float number\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divided by zero.\")\n",
    "    return a / b\n",
    "\n",
    "\n",
    "@tool\n",
    "def modulus(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Get the modulus of two numbers.\n",
    "    Args:\n",
    "        a (int): the first number\n",
    "        b (int): the second number\n",
    "    \"\"\"\n",
    "    return a % b\n",
    "\n",
    "\n",
    "@tool\n",
    "def power(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Get the power of two numbers.\n",
    "    Args:\n",
    "        a (float): the first number\n",
    "        b (float): the second number\n",
    "    \"\"\"\n",
    "    return a**b\n",
    "\n",
    "\n",
    "@tool\n",
    "def square_root(a: float) -> float | complex:\n",
    "    \"\"\"\n",
    "    Get the square root of a number.\n",
    "    Args:\n",
    "        a (float): the number to get the square root of\n",
    "    \"\"\"\n",
    "    if a >= 0:\n",
    "        return a**0.5\n",
    "    return cmath.sqrt(a)\n",
    "\n",
    "\n",
    "# ────────────────────────  generic search utils  ───────────────────────\n",
    "_SEPARATOR = \"\\n\\n---\\n\\n\"\n",
    "\n",
    "\n",
    "def _format_docs(docs: Sequence, max_chars: int = 5000) -> str:\n",
    "    \"\"\"Uniformly format loader docs for the LLM / calling agent.\"\"\"\n",
    "    if not docs:\n",
    "        return \"No results found.\"\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        meta = doc.metadata\n",
    "        snippet = doc.page_content[:max_chars].strip()\n",
    "        chunks.append(\n",
    "            f'<Document source=\"{meta.get(\"source\")}\" page=\"{meta.get(\"page\", \"\")}\">\\n'\n",
    "            f\"{snippet}\\n</Document>\"\n",
    "        )\n",
    "    return _SEPARATOR.join(chunks)\n",
    "\n",
    "\n",
    "# ─────────────────────────  wiki_search  ──────────────────────────\n",
    "\n",
    "\n",
    "@tool\n",
    "def wiki_search(query: str) -> str:\n",
    "    \"\"\"Return up to 2 Wikipedia pages about *query*.\"\"\"\n",
    "    docs = WikipediaLoader(query=query, load_max_docs=2).load()\n",
    "    return _format_docs(docs)\n",
    "\n",
    "\n",
    "# ─────────────────────────  web_search   ──────────────────────────\n",
    "\n",
    "\n",
    "# ``Document`` and ``_format_docs`` are provided by the host application.\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Public API                                                         #\n",
    "######################################################################\n",
    "@tool\n",
    "def web_search(query: str, max_results: int = 3) -> str:\n",
    "    # docstring\n",
    "    \"\"\"\n",
    "    Return up to `max_results` Google search results for *query*.\n",
    "    The output is formatted by `_format_docs`, so it matches the schema your\n",
    "    other tools already use.\n",
    "    \"\"\"\n",
    "\n",
    "    docs: List[Document] = []\n",
    "\n",
    "    try:\n",
    "        wrapper = GoogleSerperAPIWrapper(k=max_results)\n",
    "        result_json = wrapper.results(query)\n",
    "\n",
    "        # Primary path — structured organic hits\n",
    "        for hit in result_json.get(\"organic\", [])[:max_results]:\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=hit.get(\"snippet\", \"\"),\n",
    "                    metadata={\"source\": hit.get(\"link\"), \"page\": \"\"},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Fallback — single‑string answer if no organic results\n",
    "        if not docs:\n",
    "            answer = wrapper.run(query)\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=answer,\n",
    "                    metadata={\"source\": \"serper\", \"page\": \"\"},\n",
    "                )\n",
    "            )\n",
    "    except Exception:\n",
    "        # Total failure → return empty formatted structure\n",
    "        pass\n",
    "\n",
    "    return _format_docs(docs[:max_results])\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def web_search(query: str, max_results: int = 3) -> str:\n",
    "#     \"\"\"\n",
    "#     Return up to `max_results` DuckDuckGo search results for *query*.\n",
    "\n",
    "#     The output is formatted by `_format_docs`, so it matches the schema your\n",
    "#     other tools already use.\n",
    "#     \"\"\"\n",
    "#     docs = []\n",
    "#     with DDGS() as ddgs:\n",
    "#         for hit in ddgs.text(query, max_results=max_results):\n",
    "#             docs.append(\n",
    "#                 Document(\n",
    "#                     page_content=hit.get(\"body\") or hit.get(\"snippet\") or \"\",\n",
    "#                     metadata={\"source\": hit.get(\n",
    "#                         \"href\") or hit.get(\"url\"), \"page\": \"\"},\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     return _format_docs(docs)\n",
    "\n",
    "# ─────────────────────────  arxiv_search ──────────────────────────\n",
    "\n",
    "\n",
    "@tool\n",
    "def arxiv_search(query: str) -> str:\n",
    "    \"\"\"Return up to 3 recent ArXiv papers about *query*.\"\"\"\n",
    "    docs = ArxivLoader(query=query, load_max_docs=3).load()\n",
    "    return _format_docs(docs)\n",
    "\n",
    "\n",
    "# ---------- 1. Search → list of links -----------------------\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_webpage_links(url: str, same_domain_only: bool = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return all unique <a href=\"...\"> links found in the HTML at `url`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Page to scrape.\n",
    "    same_domain_only : bool, optional\n",
    "        If True, keep only links on the same domain as `url`.  Default = False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Absolute URLs, deduplicated and sorted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        html = requests.get(url, timeout=10).text\n",
    "    except Exception as exc:\n",
    "        return [f\"ERROR: fetch failed – {exc}\"]\n",
    "\n",
    "    base = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(url))\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    links: set[str] = set()\n",
    "    for tag in soup.find_all(\"a\", href=True):\n",
    "        href: str = tag[\"href\"].strip()\n",
    "        # Convert relative → absolute\n",
    "        full = urljoin(base, href)\n",
    "        if same_domain_only and urlparse(full).netloc != urlparse(url).netloc:\n",
    "            continue\n",
    "        links.add(full)\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "\n",
    "# ---------- 2. Browse → cleaned article text ----------------\n",
    "@tool\n",
    "def extract_webpage_text(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Download `url` and return the main readable text (no html, ads, nav bars).\n",
    "    Relies on trafilatura’s article extractor.\n",
    "    \"\"\"\n",
    "    raw = trafilatura.fetch_url(url)\n",
    "    if raw is None:\n",
    "        return \"🛑 Could not fetch the page.\"\n",
    "\n",
    "    text = trafilatura.extract(\n",
    "        raw,\n",
    "        include_comments=False,\n",
    "        include_tables=False,\n",
    "        include_links=False,\n",
    "    )\n",
    "    return text or \"🛑 Page fetched but no readable text found.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_links_for_match(\n",
    "    url: str,\n",
    "    keyword: str,\n",
    "    max_links: int = 100,\n",
    "    same_domain_only: bool = True,\n",
    "    case_sensitive: bool = False,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Search the content of up to `max_links` found on a webpage, and return URLs that contain the given keyword.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    url : str\n",
    "        The starting webpage to extract links from.\n",
    "    keyword : str\n",
    "        The keyword or phrase to match inside linked pages.\n",
    "    max_links : int, optional\n",
    "        Number of links to follow (default: 10).\n",
    "    same_domain_only : bool, optional\n",
    "        Only consider links from the same domain (default: True).\n",
    "    case_sensitive : bool, optional\n",
    "        Whether the keyword match should be case-sensitive.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list[str]\n",
    "        List of URLs whose content contains the keyword.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the tool's .func() to access base function\n",
    "    all_links = list_webpage_links.func(url=url, same_domain_only=same_domain_only)\n",
    "    matched_links = []\n",
    "\n",
    "    # Normalize keyword\n",
    "    kw = keyword if case_sensitive else keyword.lower()\n",
    "\n",
    "    for link in all_links[:max_links]:\n",
    "        try:\n",
    "            text = browse_webpage_link.func(link)\n",
    "            if not case_sensitive:\n",
    "                text = text.lower()\n",
    "            if kw in text:\n",
    "                matched_links.append(link)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return matched_links or [\"No matches found.\"]\n",
    "\n",
    "\n",
    "### =============== DOCUMENT PROCESSING TOOLS =============== ###\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# MarkItDown initialisation\n",
    "#   • Works out-of-the-box for PDFs, Word, PowerPoint, Excel, images, etc.\n",
    "#   • If DOCINTEL_ENDPOINT is set, heavy lifting (scanned PDFs, OCR tables…)\n",
    "#     is delegated to Azure Document Intelligence.\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "_DOCINTEL_ENDPOINT = os.getenv(\"DOCINTEL_ENDPOINT\")  # set in env if needed\n",
    "_MD = MarkItDown(enable_plugins=False, docintel_endpoint=_DOCINTEL_ENDPOINT or None)\n",
    "\n",
    "\n",
    "@tool(\"read_document\", return_direct=True)\n",
    "def read_document(file_path: str, max_pages: Optional[int] = 10) -> str:\n",
    "    \"\"\"\n",
    "    Extract plain text from **any** local document supported by MarkItDown\n",
    "    (PDF, DOCX, PPTX, XLSX, images, HTML,.py, etc.).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the document on disk.\n",
    "    max_pages : int, optional\n",
    "        Truncate output after this many pages/slides (only applies to\n",
    "        paginated formats).  If omitted, return the full text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The extracted text, or an error string that starts with\n",
    "        “[read_document error] …”.\n",
    "    \"\"\"\n",
    "    path = Path(file_path).expanduser()\n",
    "    if not path.exists():\n",
    "        return f\"[read_document error] file not found: {file_path}\"\n",
    "\n",
    "    try:\n",
    "        result = _MD.convert(str(path))\n",
    "        text = result.text_content or \"\"\n",
    "\n",
    "        # For paginated formats MarkItDown uses form-feed (\\f) between pages\n",
    "        if max_pages and max_pages > 0:\n",
    "            pages = text.split(\"\\f\")\n",
    "            text = \"\\f\".join(pages[:max_pages])\n",
    "\n",
    "        cleaned = text.strip()\n",
    "        return cleaned if cleaned else \"[read_document] no text found\"\n",
    "\n",
    "    except Exception as err:\n",
    "        return f\"[read_document error] {err}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_and_read_file(content: str, filename: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Save content to a file and return the path.\n",
    "    Args:\n",
    "        content (str): the content to save to the file\n",
    "        filename (str, optional): the name of the file. If not provided, a random name file will be created.\n",
    "    \"\"\"\n",
    "    temp_dir = Path(QUESTIONS_FILES_DIR)\n",
    "    if filename is None:\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir)\n",
    "        filepath = temp_file.name\n",
    "    else:\n",
    "        filepath = os.path.join(temp_dir, filename)\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    return f\"File saved to {filepath}. You can read this file to process its contents.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def download_file_from_url(url: str, filename: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Download a file from a URL and save it to a temporary location.\n",
    "    Args:\n",
    "        url (str): the URL of the file to download.\n",
    "        filename (str, optional): the name of the file. If not provided, a random name file will be created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse URL to get filename if not provided\n",
    "        if not filename:\n",
    "            path = urlparse(url).path\n",
    "            filename = os.path.basename(path)\n",
    "            if not filename:\n",
    "                filename = f\"downloaded_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "        # Create temporary file\n",
    "        temp_dir = Path(QUESTIONS_FILES_DIR)\n",
    "        filepath = os.path.join(temp_dir, filename)\n",
    "\n",
    "        # Download the file\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Save the file\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        return f\"File downloaded to {filepath}. You can read this file to process its contents.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading file: {str(e)}\"\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def extract_text_from_image(image_path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Extract text from an image using OCR library pytesseract (if available).\n",
    "#     Args:\n",
    "#         image_path (str): the path to the image file.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Open the image\n",
    "#         image = Image.open(image_path)\n",
    "\n",
    "#         # Extract text from the image\n",
    "#         text = pytesseract.image_to_string(image)\n",
    "\n",
    "#         return f\"Extracted text from image:\\n\\n{text}\"\n",
    "#     except Exception as e:\n",
    "#         return f\"Error extracting text from image: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def analyze_csv_file(file_path: str, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze a CSV file using pandas and answer a question about it.\n",
    "    Args:\n",
    "        file_path (str): the path to the CSV file.\n",
    "        query (str): Question about the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Run various analyses based on the query\n",
    "        result = f\"CSV file loaded with {len(df)} rows and {len(df.columns)} columns.\\n\"\n",
    "        result += f\"Columns: {', '.join(df.columns)}\\n\\n\"\n",
    "\n",
    "        # Add summary statistics\n",
    "        result += \"Summary statistics:\\n\"\n",
    "        result += str(df.describe())\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing CSV file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def analyze_excel_file(file_path: str, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze an Excel file using pandas and answer a question about it.\n",
    "    Args:\n",
    "        file_path (str): the path to the Excel file.\n",
    "        query (str): Question about the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        # Run various analyses based on the query\n",
    "        result = (\n",
    "            f\"Excel file loaded with {len(df)} rows and {len(df.columns)} columns.\\n\"\n",
    "        )\n",
    "        result += f\"Columns: {', '.join(df.columns)}\\n\\n\"\n",
    "\n",
    "        # Add summary statistics\n",
    "        result += \"Summary statistics:\\n\"\n",
    "        result += str(df.describe())\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing Excel file: {str(e)}\"\n",
    "\n",
    "\n",
    "# ─────────── vision tool ────────────────────────────\n",
    "_VISION_PROMPT = \"\"\"\n",
    "You are a GAIA-benchmark vision assistant. Return **exactly three sections**:\n",
    "\n",
    "1. Description – ≤40-word caption of the whole scene.\n",
    "2. Objects – JSON array of {\"name\": str, \"bbox\": [x0,y0,x1,y1]} for each visible item.\n",
    "3. Extracted text – verbatim text in the image or “[none]”.\n",
    "\n",
    "No extra commentary.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def _b64(path: Path) -> str:\n",
    "    with path.open(\"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "\n",
    "@tool(\"describe_image\", return_direct=True)\n",
    "def describe_image(local_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Caption a **local** image + list objects + OCR text using Groq’s\n",
    "    meta-llama/llama-4-scout-17b-16e-instruct vision model.\n",
    "\n",
    "    Steps for the agent:\n",
    "      • If you only have a URL, first call `download_file_from_url`\n",
    "        (that tool returns the tmp path). Then call this tool.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Three-section GAIA-formatted answer, or an error string.\n",
    "    \"\"\"\n",
    "    p = Path(local_path).expanduser()\n",
    "    if not p.exists():\n",
    "        return f\"[describe_image] file not found: {local_path}\"\n",
    "\n",
    "    try:\n",
    "        data_uri = f\"data:image/{p.suffix.lstrip('.').lower()};base64,{_b64(p)}\"\n",
    "        resp = _GROQ_CLIENT.chat.completions.create(\n",
    "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": _VISION_PROMPT},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as err:\n",
    "        return f\"[describe_image error] {err}\"\n",
    "\n",
    "\n",
    "# ──────────────────────── audio tool ──────────────────────────────\n",
    "\n",
    "\n",
    "@tool(\"transcribe_audio\", return_direct=True)\n",
    "def transcribe_audio(audio_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe spoken content from a local audio file using Groq Whisper-large-v3.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_path : str\n",
    "        Path to a .wav/.mp3/.m4a/.flac file on disk.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The plain-text transcription, or an error string if something fails.\n",
    "    \"\"\"\n",
    "    p = Path(audio_path).expanduser()\n",
    "    if not p.exists():\n",
    "        return f\"[transcribe_audio] file not found: {audio_path}\"\n",
    "\n",
    "    try:\n",
    "        with p.open(\"rb\") as fh:\n",
    "            resp = _GROQ_CLIENT.audio.transcriptions.create(\n",
    "                file=(p.name, fh.read()),\n",
    "                model=\"whisper-large-v3\",\n",
    "                response_format=\"text\",  # “text” → plain string in .text\n",
    "            )\n",
    "        txt = resp.text.strip() if hasattr(resp, \"text\") else str(resp).strip()\n",
    "        return txt or \"[empty transcription]\"\n",
    "\n",
    "    except Exception as err:\n",
    "        return f\"[transcribe_audio error] {err}\"\n",
    "\n",
    "\n",
    "tools = [\n",
    "    calculator,\n",
    "    wiki_search,\n",
    "    web_search,\n",
    "    arxiv_search,\n",
    "    list_webpage_links,\n",
    "    extract_webpage_text,\n",
    "    search_links_for_match,\n",
    "    save_and_read_file,\n",
    "    download_file_from_url,\n",
    "    # extract_text_from_image,\n",
    "    analyze_csv_file,\n",
    "    analyze_excel_file,\n",
    "    read_document,\n",
    "    # analyze_image,\n",
    "    # transform_image,\n",
    "    # draw_on_image,\n",
    "    # generate_simple_image,\n",
    "    # combine_images,\n",
    "    multiply,\n",
    "    add,\n",
    "    subtract,\n",
    "    divide,\n",
    "    modulus,\n",
    "    power,\n",
    "    square_root,\n",
    "    describe_image,\n",
    "    transcribe_audio,\n",
    "]\n",
    "\n",
    "\n",
    "def get_tools() -> list:\n",
    "    \"\"\"\n",
    "    Return the list of tools available for the agent.\n",
    "    This can be used to dynamically load tools in the agent.\n",
    "    \"\"\"\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c939d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Description**: A cat lounges on a white ledge.\\n\\n**Objects**: \\n[\\n  {\"name\": \"cat\", \"bbox\": [0.061, 0.184,0.969,0.874]}\\n]\\n\\n**Extracted text**: [none]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_image.invoke({\"local_path\": \"./tmp/questions_files/Cat_August_2010-4.jpg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48f169bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Document source=\"https://en.wikipedia.org/wiki/Paris\" page=\"\">\\nParis is the capital and largest city of France. With an estimated population of 2,048,472 residents in January 2025 in an area of more than 105 km2 (41 sq ...\\n</Document>\\n\\n---\\n\\n<Document source=\"https://www.coe.int/en/web/interculturalcities/paris\" page=\"\">\\nParis is the capital and most populous city of France. Situated on the Seine River, in the north of the country, it is in the centre of the Île-de-France ...\\n</Document>\\n\\n---\\n\\n<Document source=\"https://home.adelphi.edu/~ca19535/page%204.html\" page=\"\">\\nParis is the capital of France, the largest country of Europe with 550 000 km2 (65 millions inhabitants). Paris has 2.234 million inhabitants end 2011.\\n</Document>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search.invoke({\"query\": \"What is the capital of France?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
